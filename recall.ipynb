{"cells":[{"cell_type":"markdown","metadata":{},"source":["# README"]},{"cell_type":"markdown","metadata":{},"source":["关键insight\n","- 不同session id要视为**独立的交互事件**，也即每一次session id都是一个**新用户**\n","- 使用的数据集有三个: **train**（前三周），**val**（第4周），**test**（第五周）。三个集合内的session id**没有重复**\n","- 任务目标是给**定一批session的前半段，预测后半段**。\n","  - 数据中val切分成两段，**valA是前半段交互**（用于构建model input），**valB是后半段**（就是label）\n","  - test其实是第五周的**前半段**，后半段在leadboard上，用于评分，我们不可见"]},{"cell_type":"markdown","metadata":{},"source":["数据集使用：\n","- otto-validation 分割成三部分，主要用于训练时eval\n","  - train_parquet==》train\n","  - test_parquet==》valA\n","  - test_labels==》valB\n","  \n","- otto-chunk-data-inparquet-format 预估submission时就是用的test\n","    - train_parquet==>train+valA+valB\n","    - test_parquet==>test\n","- otto-valid-test-list 包含valA和test集用户的前半段交互（相当于用户冷启部分）"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 函数定义"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T14:43:58.439583Z","iopub.status.busy":"2023-01-25T14:43:58.439044Z","iopub.status.idle":"2023-01-25T14:43:58.482541Z","shell.execute_reply":"2023-01-25T14:43:58.481507Z","shell.execute_reply.started":"2023-01-25T14:43:58.439478Z"},"trusted":true},"outputs":[],"source":["VER = 1\n","import pandas as pd, numpy as np\n","import pickle, glob, gc\n","\n","from collections import Counter\n","import itertools\n","from tqdm import tqdm\n","import os\n","from functools import partial\n","\n","# multiprocessing\n","import psutil\n","N_CORES = min(psutil.cpu_count(),32)     # Available CPU cores\n","print(f\"N Cores : {N_CORES}\")\n","from multiprocessing import Pool"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T14:43:58.635392Z","iopub.status.busy":"2023-01-25T14:43:58.634447Z","iopub.status.idle":"2023-01-25T14:43:58.641431Z","shell.execute_reply":"2023-01-25T14:43:58.639627Z","shell.execute_reply.started":"2023-01-25T14:43:58.635346Z"},"trusted":true},"outputs":[],"source":["type_labels = {'clicks':0, 'carts':1, 'orders':2}\n","type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n","RECALL_NUM = 100\n","CHUNK_NUM = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T14:43:58.835585Z","iopub.status.busy":"2023-01-25T14:43:58.835133Z","iopub.status.idle":"2023-01-25T14:43:58.849372Z","shell.execute_reply":"2023-01-25T14:43:58.848086Z","shell.execute_reply.started":"2023-01-25T14:43:58.835546Z"},"trusted":true},"outputs":[],"source":["def df_parallelize_run(func, t_split):\n","    num_cores = np.min([N_CORES, len(t_split)])\n","    pool = Pool(num_cores)\n","    df = pool.map(func, t_split)\n","    pool.close()\n","    pool.join()\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T14:43:58.940607Z","iopub.status.busy":"2023-01-25T14:43:58.940192Z","iopub.status.idle":"2023-01-25T14:43:58.94969Z","shell.execute_reply":"2023-01-25T14:43:58.948159Z","shell.execute_reply.started":"2023-01-25T14:43:58.940574Z"},"trusted":true},"outputs":[],"source":["def load_files(files):  \n","    dfs = []\n","    for e, chunk_file in enumerate(glob.glob(files)):\n","        chunk = pd.read_parquet(chunk_file)\n","        chunk.ts = (chunk.ts/1000).astype('int32')\n","        chunk.session = chunk.session.astype('int32')\n","        chunk.aid = chunk.aid.astype('int32')\n","        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n","        dfs.append(chunk)\n","    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n","def pqt_to_dict(df):\n","    '''\n","    join into form like below\n","    aid1:[aid2,aid3,aid4],\n","    aid2:[aid1,aid3,aid4]\n","    '''\n","    return df.groupby('aid_x').aid_y.apply(list).to_dict()"]},{"cell_type":"markdown","metadata":{},"source":["召回click的函数"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T14:43:59.218259Z","iopub.status.busy":"2023-01-25T14:43:59.217841Z","iopub.status.idle":"2023-01-25T14:43:59.228839Z","shell.execute_reply":"2023-01-25T14:43:59.227749Z","shell.execute_reply.started":"2023-01-25T14:43:59.218224Z"},"trusted":true},"outputs":[],"source":["def suggest_clicks(df,recall_num=50):\n","    session = df[0]\n","    aids = df[1]\n","    types = df[2]\n","    unique_aids = list(dict.fromkeys(aids[::-1] ))\n","    # RERANK CANDIDATES USING WEIGHTS\n","    if len(unique_aids)>=recall_num:\n","        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n","        aids_temp = Counter() \n","        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n","        # 对近期交互过的aid，按照时间远近以及交互类型进行加权\n","        for aid,w,t in zip(aids,weights,types): \n","            aids_temp[aid] += w * type_weight_multipliers[t]\n","        # 然后取top \n","        sorted_aids = [k for k,v in aids_temp.most_common(recall_num)]\n","        return session, sorted_aids \n","\n","    # 遍历每一个aid，取click共现的top20\n","    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n","    # 排序\n","    top_aids2 = [aid2 for aid2, recall_num in Counter(aids2).most_common(recall_num) if aid2 not in unique_aids]    \n","    result = unique_aids + top_aids2[:recall_num - len(unique_aids)]\n","    # debug用\n","    #     if np.random.uniform()<0.01:\n","    #         print(f\"len before use top {len(result)}\")\n","    # 补充高热\n","    return session, result + list(top_clicks)[:recall_num-len(result)]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 召回buy和cart的函数\n","（因为cart和buy行为都很稀疏，共现矩阵把两个行为放在一起计算了）"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T14:43:59.691382Z","iopub.status.busy":"2023-01-25T14:43:59.689958Z","iopub.status.idle":"2023-01-25T14:43:59.705991Z","shell.execute_reply":"2023-01-25T14:43:59.704571Z","shell.execute_reply.started":"2023-01-25T14:43:59.691318Z"},"trusted":true},"outputs":[],"source":["def suggest_buys(df,recall_num=50):\n","    # USE USER HISTORY AIDS AND TYPES\n","    session = df[0]\n","    aids = df[1]\n","    types = df[2]\n","\n","    unique_aids = list(dict.fromkeys(aids[::-1] ))\n","    # 包括cart和order两种行为\n","    unique_buys = list(dict.fromkeys( [f for i, f in enumerate(aids) if types[i] in [1, 2]][::-1] ))\n","    \n","    if len(unique_aids)>=recall_num:        \n","        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n","        aids_temp = Counter() \n","        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n","        for aid,w,t in zip(aids,weights,types): \n","            aids_temp[aid] += w * type_weight_multipliers[t]\n","        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n","        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n","        for aid in aids3: aids_temp[aid] += 0.1\n","        sorted_aids = [k for k,v in aids_temp.most_common(recall_num)]\n","        return session, sorted_aids\n","            \n","    # 用\"CART ORDER\" CO-VISITATION MATRIX\n","    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n","    # 用\"BUY2BUY\" CO-VISITATION MATRIX\n","    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n","    # 排序\n","    top_aids2 = [aid2 for aid2, recall_num in Counter(aids2 + aids3).most_common(recall_num) if aid2 not in unique_aids] \n","    result = unique_aids + top_aids2[:recall_num - len(unique_aids)]\n","#     if np.random.uniform()<0.01:\n","#         print(f\"len before use top {len(result)}\")\n","    # 用高热补充\n","    return session, result + list(top_orders)[:recall_num-len(result)]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 分chunk执行"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T14:44:00.548943Z","iopub.status.busy":"2023-01-25T14:44:00.547794Z","iopub.status.idle":"2023-01-25T14:44:00.563174Z","shell.execute_reply":"2023-01-25T14:44:00.561923Z","shell.execute_reply.started":"2023-01-25T14:44:00.54888Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["def save_parquet(df,folder,name):\n","    if folder==\"\":\n","        folder=\"./\"\n","    else:\n","        folder=f\"./{folder}\"\n","    if not os.path.exists(folder):\n","        os.mkdir(folder)\n","    df.to_parquet(f\"{folder}/{name}.pqt\")\n","    print(f\"Save file name = {folder}/{name}.pqt\")\n","\n","def predict_by_chunk(bysession_list, folder, chunk_num=10, recall_num=50):\n","    # 分chunk执行\n","    def split(list_a, chunk_num):\n","        chunk_size = len(list_a)//chunk_num\n","        for i in range(0, len(list_a), chunk_size):\n","            yield list_a[i:i + chunk_size]\n","    \n","    clicks = []\n","    buys = []\n","    for i,sub_list in tqdm(enumerate(split(bysession_list, chunk_num=chunk_num))):\n","        ## clicks\n","        temp = df_parallelize_run(partial(suggest_clicks,recall_num=recall_num), sub_list)\n","        clicks.append(pd.Series([f[1]  for f in temp], index=[f[0] for f in temp]).to_frame().rename(columns= {0: 'aid_list'}))\n","\n","        ## buys\n","        temp = df_parallelize_run(partial(suggest_buys,recall_num=recall_num), sub_list)\n","        buys.append(pd.Series([f[1]  for f in temp], index=[f[0] for f in temp]).to_frame().rename(columns= {0: 'aid_list'}))\n","        \n","    # 将chunk拼接好\n","    clicks = pd.concat(clicks)\n","    orders = pd.concat(buys)\n","    print(f\"Get clicks candidates avg len = {np.mean(clicks.aid_list.apply(lambda x:len(x)))}\")\n","    print(f\"Get buys candidates avg len = {np.mean(orders.aid_list.apply(lambda x:len(x)))}\")\n","    \n","    # 存储\n","    save_parquet(clicks, folder, name=\"clicks_candidates\")\n","    save_parquet(orders, folder, name=\"orders_candidates\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T14:44:00.745889Z","iopub.status.busy":"2023-01-25T14:44:00.745434Z","iopub.status.idle":"2023-01-25T14:44:00.753443Z","shell.execute_reply":"2023-01-25T14:44:00.752217Z","shell.execute_reply.started":"2023-01-25T14:44:00.74585Z"},"trusted":true},"outputs":[],"source":["# 获取explode后的结果\n","def get_explode_candidates(df,folder,file_name,chunk_num=10):\n","    user_base_explode_chunks = []\n","    for i,chunk in tqdm(enumerate(np.array_split(pd.DataFrame(df.index.unique(),columns=['session']), chunk_num))):\n","        user_base_explode_chunks.append(df[df.index.isin(chunk.session)].explode(\"aid_list\").rename(columns={\"aid_list\":\"aid\"}))\n","    user_base_exploded = pd.concat(user_base_explode_chunks)\n","    save_parquet(user_base_exploded,folder,file_name)"]},{"cell_type":"markdown","metadata":{},"source":["# Valid用户召回"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## load数据\n","val部分用户前半周的交互，用于召回的输入"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T13:22:33.168803Z","iopub.status.busy":"2023-01-25T13:22:33.16757Z","iopub.status.idle":"2023-01-25T13:22:48.305851Z","shell.execute_reply":"2023-01-25T13:22:48.30379Z","shell.execute_reply.started":"2023-01-25T13:22:33.168752Z"},"trusted":true},"outputs":[],"source":["%%time\n","PIECES = 5\n","valid_bysession_list = []\n","for PART in range(PIECES):\n","    with open(f'../input/otto-valid-test-list/valid_group_tolist_{PART}_{VER}.pkl', 'rb') as f:\n","        valid_bysession_list.extend(pickle.load(f))\n","print(len(valid_bysession_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T13:22:48.309751Z","iopub.status.busy":"2023-01-25T13:22:48.309203Z","iopub.status.idle":"2023-01-25T13:24:59.18156Z","shell.execute_reply":"2023-01-25T13:24:59.179996Z","shell.execute_reply.started":"2023-01-25T13:22:48.309704Z"},"trusted":true},"outputs":[],"source":["%%time\n","DISK_PIECES = 4\n","# LOAD THREE CO-VISITATION MATRICES\n","top_20_clicks = pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_20_valid_clicks_v{VER}_0.pqt') )\n","for k in range(1, DISK_PIECES): \n","    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_20_valid_clicks_v{VER}_{k}.pqt') ) )\n","\n","top_20_buys = pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_15_valid_carts_orders_v{VER}_0.pqt') )\n","for k in range(1, DISK_PIECES): \n","    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_15_valid_carts_orders_v{VER}_{k}.pqt') ) )\n","\n","top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_15_valid_buy2buy_v{VER}_0.pqt') )\n","\n","# TOP CLICKS AND ORDERS IN ValidA\n","valid = load_files('../input/otto-validation/test_parquet/*')\n","top_clicks = valid.loc[valid['type']==type_labels['clicks'], 'aid'].value_counts().index.values[:RECALL_NUM]\n","top_orders = valid.loc[valid['type']==type_labels['orders'], 'aid'].value_counts().index.values[:RECALL_NUM]\n","del valid\n","_ = gc.collect()\n","\n","print('Here are size of our 3 co-visitation matrices:')\n","print(f\"top_20_clicks = {len(top_20_clicks)}, top_20_buy2buy = {len(top_20_buy2buy)}, top_20_buys = {len(top_20_buys)}\")\n","print(f\"top_clicks = {len(top_clicks)}, top_orders = {len(top_orders)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 执行召回"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T13:24:59.183362Z","iopub.status.busy":"2023-01-25T13:24:59.182968Z","iopub.status.idle":"2023-01-25T13:32:41.828119Z","shell.execute_reply":"2023-01-25T13:32:41.826462Z","shell.execute_reply.started":"2023-01-25T13:24:59.183329Z"},"trusted":true},"outputs":[],"source":["%%time\n","predict_by_chunk(valid_bysession_list,\"val\",chunk_num=CHUNK_NUM,recall_num=RECALL_NUM)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## explode"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_click_candidates = pd.read_parquet('./val/clicks_candidates.pqt')\n","get_explode_candidates(val_click_candidates,'val','clicks_exploded_candidates',chunk_num=CHUNK_NUM)\n","del val_click_candidates\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T13:32:41.833575Z","iopub.status.busy":"2023-01-25T13:32:41.83313Z"},"trusted":true},"outputs":[],"source":["val_order_candidates = pd.read_parquet('./val/orders_candidates.pqt')\n","get_explode_candidates(val_order_candidates,'val','orders_exploded_candidates',chunk_num=CHUNK_NUM)\n","del val_order_candidates\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# FREE MEMORY\n","del valid_bysession_list\n","del top_20_clicks, top_20_buy2buy, top_20_buys, top_clicks, top_orders\n","_ = gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Test用户召回"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## load数据\n","load物料，co-visitation矩阵，test set时期的近期高热。作为候选"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","# 读取test的共现矩阵和高热\n","DISK_PIECES = 4\n","top_20_clicks = pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_20_test_clicks_v{VER}_0.pqt') )\n","for k in range(1, DISK_PIECES): \n","    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_20_test_clicks_v{VER}_{k}.pqt') ) )\n","\n","top_20_buys = pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_15_test_carts_orders_v{VER}_0.pqt') )\n","for k in range(1, DISK_PIECES): \n","    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_15_test_carts_orders_v{VER}_{k}.pqt') ) )\n","    \n","top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'../input/otto-co-visitation-matrices/top_15_test_buy2buy_v{VER}_0.pqt') )\n","\n","# TOP CLICKS AND ORDERS IN TEST\n","test = load_files('../input/otto-chunk-data-inparquet-format/test_parquet/*')\n","top_clicks = test.loc[test['type']==type_labels['clicks'], 'aid'].value_counts().index.values[:RECALL_NUM]\n","top_orders = test.loc[test['type']==type_labels['orders'], 'aid'].value_counts().index.values[:RECALL_NUM]\n","del test\n","_ = gc.collect()\n","\n","print('Here are size of our 3 co-visitation matrices:')\n","print(f\"top_20_clicks = {len(top_20_clicks)}, top_20_buy2buy = {len(top_20_buy2buy)}, top_20_buys = {len(top_20_buys)}\")\n","print(f\"top_clicks = {len(top_clicks)}, top_orders = {len(top_orders)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","# 读取\n","PIECES = 5\n","test_bysession_list = []\n","for PART in range(PIECES):\n","    with open(f'../input/otto-valid-test-list/test_group_tolist_{PART}_{VER}.pkl', 'rb') as f:\n","        test_bysession_list.extend(pickle.load(f))\n","print(f\"Test user cnt = {len(test_bysession_list)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 执行召回"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","# 同时召回[click]和[buy+cart]两类\n","predict_by_chunk(test_bysession_list,\"test\",chunk_num=CHUNK_NUM,recall_num=RECALL_NUM)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## explode"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-25T14:44:05.986584Z","iopub.status.busy":"2023-01-25T14:44:05.985305Z","iopub.status.idle":"2023-01-25T14:46:15.46886Z","shell.execute_reply":"2023-01-25T14:46:15.467264Z","shell.execute_reply.started":"2023-01-25T14:44:05.98653Z"},"trusted":true},"outputs":[],"source":["test_click_candidates = pd.read_parquet('./test/clicks_candidates.pqt')\n","get_explode_candidates(test_click_candidates,'test','clicks_exploded_candidates',chunk_num=CHUNK_NUM)\n","del test_click_candidates\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_order_candidates = pd.read_parquet('./test/orders_candidates.pqt')\n","get_explode_candidates(test_order_candidates,'test','orders_exploded_candidates',chunk_num=CHUNK_NUM)\n","del test_order_candidates\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# FREE MEMORY\n","del test_bysession_list\n","del top_20_clicks, top_20_buy2buy, top_20_buys, top_clicks, top_orders\n","_ = gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Metric\n","评估单纯效果"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["benchmark = {\"clicks\":0.5255597442145808, \"carts\":0.4093328152483512, \"orders\":0.6487936598117477, \"all\":.5646320148830121}\n","weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n","\n","valid_labels = pd.read_parquet('../input/otto-validation/test_labels.parquet')\n","\n","def hits(b):\n","    # b[0] : session id\n","    # b[1] : ground truth\n","    # b[2] : aids prediction \n","    return b[0], len(set(b[1]).intersection(set(b[2]))), np.clip(len(b[1]), 0, 20)\n","\n","def otto_metric_piece(values, typ, verbose=True):\n","    '''\n","    values==> session:[aid list]\n","    '''\n","    c1 = pd.DataFrame(values, columns=[\"labels\"]).reset_index().rename({\"index\":\"session\"}, axis=1)\n","    a = valid_labels.loc[valid_labels['type']==typ].merge(c1, how='left', on=['session'])\n","\n","    b=[[a0, a1, a2] for a0, a1, a2 in zip(a[\"session\"], a[\"ground_truth\"], a[\"labels\"])]\n","    c = df_parallelize_run(hits, b)\n","    c = np.array(c)\n","    \n","    recall = c[:,1].sum() / c[:,2].sum()\n","    \n","    print('{} recall = {:.5f} (vs {:.5f} in benchmark)'.format(typ ,recall, benchmark[typ]))\n","    \n","    return recall\n","\n","def otto_metric(clicks, carts, orders, verbose = True):\n","    \n","    score = 0\n","    score += weights[\"clicks\"] * otto_metric_piece(clicks, \"clicks\", verbose = verbose)\n","    score += weights[\"carts\"] * otto_metric_piece(carts, \"carts\", verbose = verbose)\n","    score += weights[\"orders\"] * otto_metric_piece(orders, \"orders\", verbose = verbose)\n","    \n","    if verbose:\n","        print('=============')\n","        print('Overall Recall = {:.5f} (vs {:.5f} in benchmark)'.format(score, benchmark[\"all\"]))\n","        print('=============')\n","    \n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["val_clicks = pd.read_parquet(\"./val/clicks_candidates.pqt\").rename(columns={'aid_list':'labels'})\n","val_orders = pd.read_parquet(\"./val/orders_candidates.pqt\").rename(columns={'aid_list':'labels'})\n","val_clicks.labels = val_clicks.labels.apply(lambda x: x[:20])\n","val_orders.labels = val_orders.labels.apply(lambda x: x[:20])\n","# 计算recall rate和总分数\n","_ = otto_metric(val_clicks, val_orders, val_orders)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 提交\n","直接提交基于recall的test。用于debug"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import pandas as pd\n","# import numpy as np\n","# def to_submission_format(df,suffix):\n","#     sub = df.reset_index().rename(columns={'index':'session_type','aid_list':'labels'})\n","#     sub.labels = sub.labels.apply(lambda x: \" \".join(map(str,x[:20])))\n","#     sub.session_type = sub.session_type.astype('str')+ f'_{suffix}'\n","#     return sub\n","# test_clicks = pd.read_parquet(\"./test/clicks_candidates.pqt\")\n","# test_orders = pd.read_parquet(\"./test/orders_candidates.pqt\")\n","# clicks_sub = to_submission_format(test_clicks,'clicks')\n","# carts_sub = to_submission_format(test_orders,'carts')\n","# orders_sub = to_submission_format(test_orders,'orders')\n","# sub = pd.concat([clicks_sub, carts_sub, orders_sub])\n","# assert sub.shape[0]==1671803*3\n","\n","# sub.to_csv(\"submission.csv\", index=False)\n","# sub.head(10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 创建数据集"]}],"metadata":{"kernelspec":{"display_name":"kaggle","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n[GCC 9.4.0]"},"vscode":{"interpreter":{"hash":"a3218bb28529b36eec24a9024c4e375ca4dd97d153ce39508d999fa46c0f46b5"}}},"nbformat":4,"nbformat_minor":4}
